{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from jira import JIRA\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database setup completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from src.utils.constants import MODEL_OPENAI\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# Get database connection parameters from environment variables\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "port = os.getenv(\"POSTGRES_PORT\")\n",
    "dbname = os.getenv(\"POSTGRES_DB\")\n",
    "user = os.getenv(\"POSTGRES_USER\")\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "model = os.getenv(\"MODEL\")\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{dbname}'\")\n",
    "if not cursor.fetchone():\n",
    "    cursor.execute(f\"CREATE DATABASE {dbname}\")\n",
    "\n",
    "# Connect to the newly created database\n",
    "conn.close()\n",
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    dbname=dbname,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Enable pgvector extension\n",
    "cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "# Create tables\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS jira_issues (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    issue_key TEXT UNIQUE NOT NULL,\n",
    "    summary TEXT NOT NULL,\n",
    "    description TEXT,\n",
    "    region TEXT,\n",
    "    status TEXT NOT NULL,\n",
    "    created_at TIMESTAMP NOT NULL,\n",
    "    updated_at TIMESTAMP NOT NULL,\n",
    "    resolved_at TIMESTAMP,\n",
    "    assignee TEXT,\n",
    "    reporter TEXT,\n",
    "    reporter_email TEXT,\n",
    "    issue_type TEXT NOT NULL,\n",
    "    priority TEXT,\n",
    "    labels TEXT[],\n",
    "    project TEXT NOT NULL,\n",
    "    components TEXT[],\n",
    "    affects_versions TEXT[],\n",
    "    fix_versions TEXT[],\n",
    "    resolution TEXT,\n",
    "    votes INTEGER,\n",
    "    remaining_estimate INTERVAL,  -- in minutes\n",
    "    time_spent INTERVAL,          -- in minutes\n",
    "    original_estimate INTERVAL,   -- in minutes\n",
    "    rank TEXT,\n",
    "    main_category TEXT,\n",
    "    sub_category TEXT,\n",
    "    partner_names TEXT,\n",
    "    relevant_departments TEXT,\n",
    "    request_category TEXT,\n",
    "    request_type TEXT,\n",
    "    request_language TEXT,\n",
    "    resolution_action TEXT,\n",
    "    source TEXT,\n",
    "    time_to_first_response TEXT,\n",
    "    time_to_resolution TEXT,\n",
    "    status_category_changed TEXT,\n",
    "    date_of_first_response TEXT,\n",
    "    raw_data JSONB\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS jira_comments (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    issue_key TEXT REFERENCES jira_issues(issue_key),\n",
    "    author TEXT,\n",
    "    body TEXT NOT NULL,\n",
    "    created_at TIMESTAMP NOT NULL,\n",
    "    updated_at TIMESTAMP NOT NULL,\n",
    "    comment_date TEXT,      -- Additional field to capture date in text format\n",
    "    raw_data JSONB\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS jira_attachments (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    issue_key TEXT REFERENCES jira_issues(issue_key),\n",
    "    filename TEXT NOT NULL,\n",
    "    content_type TEXT,\n",
    "    size INTEGER,\n",
    "    created_at TIMESTAMP,\n",
    "    author TEXT,\n",
    "    raw_data JSONB\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# for nomic-embed\n",
    "emb_size = 768\n",
    "# for openai\n",
    "if model == MODEL_OPENAI:\n",
    "    emb_size = 1536\n",
    "cursor.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS embeddings (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    source_type TEXT NOT NULL,\n",
    "    source_id TEXT NOT NULL,\n",
    "    content TEXT NOT NULL,\n",
    "    embedding vector({emb_size}),\n",
    "    chunk_number INTEGER,\n",
    "    UNIQUE(source_type, source_id, chunk_number)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Create indices\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_jira_issues_issue_key ON jira_issues(issue_key)\")\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_jira_issues_project ON jira_issues(project)\")\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_jira_issues_main_category ON jira_issues(main_category)\")\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_jira_issues_status ON jira_issues(status)\")\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_jira_comments_issue_key ON jira_comments(issue_key)\")\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_jira_attachments_issue_key ON jira_attachments(issue_key)\")\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_embeddings_source ON embeddings(source_type, source_id)\")\n",
    "# ivfflat -> more accurate; hnsw -> faster less accurate\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_embeddings_embedding ON embeddings USING ivfflat (embedding vector_cosine_ops)\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database setup completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from jira import JIRA\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# Jira connection parameters\n",
    "jira_url = os.getenv(\"JIRA_URL\")\n",
    "jira_email = os.getenv(\"JIRA_EMAIL\")\n",
    "jira_api_token = os.getenv(\"JIRA_API_TOKEN\")\n",
    "\n",
    "# Database connection parameters\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "port = os.getenv(\"POSTGRES_PORT\")\n",
    "dbname = os.getenv(\"POSTGRES_DB\")\n",
    "user = os.getenv(\"POSTGRES_USER\")\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "\n",
    "def connect_to_jira():\n",
    "    \"\"\"Establish connection to Jira.\"\"\"\n",
    "    return JIRA(\n",
    "        server=jira_url,\n",
    "        basic_auth=(jira_email, jira_api_token)\n",
    "    )\n",
    "\n",
    "def parse_time_string(time_str):\n",
    "    \"\"\"Parse time string in the format HH:MM:SS into minutes.\"\"\"\n",
    "    if not time_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) == 2:  # HH:MM\n",
    "            hours, minutes = map(int, parts)\n",
    "            return hours * 60 + minutes\n",
    "        elif len(parts) == 3:  # HH:MM:SS\n",
    "            hours, minutes, seconds = map(int, parts)\n",
    "            return hours * 60 + minutes + (seconds // 60)\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def extract_and_load_issues(project_key, max_results=1000):\n",
    "    \"\"\"Extract issues from Jira and load them into the database.\"\"\"\n",
    "    jira = connect_to_jira()\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # JQL query to get all issues from a project\n",
    "    jql = f\"project = {project_key} ORDER BY created DESC\"\n",
    "    \n",
    "    # Get issues\n",
    "    issues = jira.search_issues(jql, maxResults=max_results)\n",
    "    \n",
    "    # Prepare data for batch insert\n",
    "    issue_data = []\n",
    "    for issue in issues:\n",
    "        # Extract basic issue data\n",
    "        issue_dict = {\n",
    "            \"issue_key\": issue.key,\n",
    "            \"summary\": issue.fields.summary,\n",
    "            \"description\": issue.fields.description or \"\",\n",
    "            \"status\": issue.fields.status.name,\n",
    "            \"created_at\": issue.fields.created,\n",
    "            \"updated_at\": issue.fields.updated,\n",
    "            \"resolved_at\": getattr(issue.fields, 'resolutiondate', None),\n",
    "            \"assignee\": issue.fields.assignee.displayName if issue.fields.assignee else None,\n",
    "            \"reporter\": issue.fields.reporter.displayName if issue.fields.reporter else None,\n",
    "            \"reporter_email\": issue.fields.reporter.emailAddress if issue.fields.reporter else None,\n",
    "            \"issue_type\": issue.fields.issuetype.name,\n",
    "            \"priority\": issue.fields.priority.name if issue.fields.priority else None,\n",
    "            \"labels\": issue.fields.labels,\n",
    "            \"project\": issue.fields.project.key,\n",
    "            \"components\": [c.name for c in issue.fields.components] if hasattr(issue.fields, 'components') else [],\n",
    "            \"affects_versions\": [v.name for v in issue.fields.versions] if hasattr(issue.fields, 'versions') else [],\n",
    "            \"fix_versions\": [v.name for v in issue.fields.fixVersions] if hasattr(issue.fields, 'fixVersions') else [],\n",
    "            \"resolution\": issue.fields.resolution.name if issue.fields.resolution else None,\n",
    "            \"votes\": issue.fields.votes.votes if hasattr(issue.fields.votes, 'votes') else 0,\n",
    "            \"remaining_estimate\": parse_time_string(getattr(issue.fields, 'timeestimate', None)),\n",
    "            \"time_spent\": parse_time_string(getattr(issue.fields, 'timespent', None)),\n",
    "            \"original_estimate\": parse_time_string(getattr(issue.fields, 'timeoriginalestimate', None)),\n",
    "            \"rank\": getattr(issue.fields, 'customfield_10019', None),  \n",
    "            \"raw_data\": json.dumps(issue.raw)\n",
    "        }\n",
    "        \n",
    "        # Extract custom fields\n",
    "        try:\n",
    "            issue_dict[\"region\"] = getattr(issue.fields, 'customfield_10103', None)\n",
    "            issue_dict[\"main_category\"] = getattr(issue.fields, 'customfield_10101', None)[0]\n",
    "            issue_dict[\"sub_category\"] = getattr(issue.fields, 'customfield_10096', None)[0]\n",
    "            issue_dict[\"request_category\"] = getattr(issue.fields, 'customfield_10098', None)\n",
    "            issue_dict[\"partner_names\"] = getattr(issue.fields, 'customfield_10108', None)\n",
    "            issue_dict[\"request_language\"] = getattr(issue.fields, 'customfield_10109', None)\n",
    "            issue_dict[\"source\"] = getattr(issue.fields, 'customfield_10111', None)\n",
    "            issue_dict[\"time_to_first_response\"] = getattr(issue.fields, 'customfield_10094', None)\n",
    "            issue_dict[\"relevant_departments\"] = getattr(issue.fields, 'customfield_10104', None)\n",
    "            issue_dict[\"resolution_action\"] = getattr(issue.fields, 'customfield_10106', None)\n",
    "            issue_dict[\"time_to_resolution\"] = getattr(issue.fields, 'customfield_10107', None)\n",
    "            issue_dict[\"status_category_changed\"] = getattr(issue.fields, 'customfield_10100', None)\n",
    "            issue_dict[\"date_of_first_response\"] = getattr(issue.fields, 'customfield_10095', None)\n",
    "            issue_dict[\"request_type\"] = getattr(issue.fields, 'customfield_10099', None)\n",
    "        except AttributeError:\n",
    "            # Handle missing fields\n",
    "            pass\n",
    "        \n",
    "        # Flatten the data for database insertion\n",
    "        db_record = (\n",
    "            issue_dict[\"issue_key\"],\n",
    "            issue_dict[\"summary\"],\n",
    "            issue_dict[\"description\"],\n",
    "            issue_dict[\"status\"],\n",
    "            issue_dict[\"created_at\"],\n",
    "            issue_dict[\"updated_at\"],\n",
    "            issue_dict[\"resolved_at\"],\n",
    "            issue_dict[\"assignee\"],\n",
    "            issue_dict[\"reporter\"],\n",
    "            issue_dict[\"reporter_email\"],\n",
    "            issue_dict[\"issue_type\"],\n",
    "            issue_dict[\"priority\"],\n",
    "            issue_dict[\"labels\"],\n",
    "            issue_dict[\"project\"],\n",
    "            issue_dict[\"components\"],\n",
    "            issue_dict[\"affects_versions\"],\n",
    "            issue_dict[\"fix_versions\"],\n",
    "            issue_dict[\"resolution\"],\n",
    "            issue_dict[\"votes\"],\n",
    "            issue_dict[\"remaining_estimate\"],\n",
    "            issue_dict[\"time_spent\"],\n",
    "            issue_dict[\"original_estimate\"],\n",
    "            issue_dict[\"rank\"],\n",
    "            issue_dict.get(\"main_category\"),\n",
    "            issue_dict.get(\"sub_category\"),\n",
    "            issue_dict.get(\"partner_names\"),\n",
    "            issue_dict.get(\"relevant_departments\"),\n",
    "            issue_dict.get(\"request_category\"),\n",
    "            issue_dict.get(\"request_type\"),\n",
    "            issue_dict.get(\"request_language\"),\n",
    "            issue_dict.get(\"resolution_action\"),\n",
    "            issue_dict.get(\"source\"),\n",
    "            issue_dict.get(\"time_to_first_response\"),\n",
    "            issue_dict.get(\"time_to_resolution\"),\n",
    "            issue_dict.get(\"status_category_changed\"),\n",
    "            issue_dict.get(\"date_of_first_response\"),\n",
    "            issue_dict[\"raw_data\"]\n",
    "        )\n",
    "        \n",
    "        issue_data.append(db_record)\n",
    "    \n",
    "    # Batch insert issues\n",
    "    if issue_data:\n",
    "        execute_values(\n",
    "            cursor,\n",
    "            \"\"\"\n",
    "            INSERT INTO jira_issues \n",
    "            (issue_key, summary, description, status, created_at, updated_at, \n",
    "             resolved_at, assignee, reporter, reporter_email, issue_type, priority, \n",
    "             labels, project, components, affects_versions, fix_versions, resolution, \n",
    "             votes, remaining_estimate, time_spent, original_estimate, rank, \n",
    "             main_category, sub_category, partner_names, relevant_departments, \n",
    "             request_category, request_type, request_language, resolution_action, \n",
    "             source, time_to_first_response, time_to_resolution, status_category_changed, \n",
    "             date_of_first_response, raw_data)\n",
    "            VALUES %s\n",
    "            ON CONFLICT (issue_key) DO UPDATE SET\n",
    "                summary = EXCLUDED.summary,\n",
    "                description = EXCLUDED.description,\n",
    "                status = EXCLUDED.status,\n",
    "                updated_at = EXCLUDED.updated_at,\n",
    "                resolved_at = EXCLUDED.resolved_at,\n",
    "                assignee = EXCLUDED.assignee,\n",
    "                priority = EXCLUDED.priority,\n",
    "                labels = EXCLUDED.labels,\n",
    "                resolution = EXCLUDED.resolution,\n",
    "                votes = EXCLUDED.votes,\n",
    "                remaining_estimate = EXCLUDED.remaining_estimate,\n",
    "                time_spent = EXCLUDED.time_spent,\n",
    "                rank = EXCLUDED.rank,\n",
    "                main_category = EXCLUDED.main_category,\n",
    "                sub_category = EXCLUDED.sub_category,\n",
    "                request_category = EXCLUDED.request_category,\n",
    "                raw_data = EXCLUDED.raw_data\n",
    "            \"\"\",\n",
    "            issue_data\n",
    "        )\n",
    "    \n",
    "    # Extract and insert comments and attachments for each issue\n",
    "    for issue in issues:\n",
    "        extract_and_load_comments(issue.key, jira, cursor)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Extracted {len(issues)} issues from Jira project {project_key}\")\n",
    "\n",
    "def extract_and_load_comments(issue_key, jira, cursor):\n",
    "    \"\"\"Extract comments for a Jira issue and load them into the database.\"\"\"\n",
    "    issue = jira.issue(issue_key)\n",
    "    comments = issue.fields.comment.comments\n",
    "    \n",
    "    comment_data = []\n",
    "    for comment in comments:\n",
    "        # Extract creation date text from the PDF format (e.g., \"09/03/2024 11:49\")\n",
    "        comment_date = None\n",
    "        if hasattr(comment, 'body') and comment.body:\n",
    "            date_match = re.search(r'(\\d{2}/\\d{2}/\\d{4}\\s\\d{2}:\\d{2})', comment.body)\n",
    "            if date_match:\n",
    "                comment_date = date_match.group(1)\n",
    "        \n",
    "        comment_dict = {\n",
    "            \"issue_key\": issue_key,\n",
    "            \"author\": 'Unknown',#comment.author.displayName if hasattr(comment.author, 'displayName') else None,\n",
    "            \"body\": comment.body,\n",
    "            \"created_at\": comment.created,\n",
    "            \"updated_at\": comment.updated,\n",
    "            \"comment_date\": comment_date,  # Add the extracted date\n",
    "            \"raw_data\": json.dumps(comment.raw)\n",
    "        }\n",
    "        \n",
    "        comment_data.append(\n",
    "            (\n",
    "                comment_dict[\"issue_key\"],\n",
    "                comment_dict[\"author\"],\n",
    "                comment_dict[\"body\"],\n",
    "                comment_dict[\"created_at\"],\n",
    "                comment_dict[\"updated_at\"],\n",
    "                comment_dict[\"comment_date\"],\n",
    "                comment_dict[\"raw_data\"]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Delete existing comments for this issue and insert new ones\n",
    "    cursor.execute(\"DELETE FROM jira_comments WHERE issue_key = %s\", (issue_key,))\n",
    "    \n",
    "    if comment_data:\n",
    "        execute_values(\n",
    "            cursor,\n",
    "            \"\"\"\n",
    "            INSERT INTO jira_comments \n",
    "            (issue_key, author, body, created_at, updated_at, comment_date, raw_data)\n",
    "            VALUES %s\n",
    "            \"\"\",\n",
    "            comment_data\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import argparse\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='Extract Jira data to PostgreSQL')\n",
    "#     parser.add_argument('--project', required=True, help='Jira project key')\n",
    "#     parser.add_argument('--max_results', type=int, default=1000, help='Maximum number of issues to extract')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     extract_and_load_issues(args.project, args.max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 issues from Jira project CAFBSS\n"
     ]
    }
   ],
   "source": [
    "extract_and_load_issues('CAFBSS', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import psycopg2\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import ollama\n",
    "from src.utils.constants import MODEL_OPENAI\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Database connection parameters\n",
    "host = os.getenv(\"POSTGRES_HOST\")\n",
    "port = os.getenv(\"POSTGRES_PORT\")\n",
    "dbname = os.getenv(\"POSTGRES_DB\")\n",
    "user = os.getenv(\"POSTGRES_USER\")\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "model = os.getenv(\"MODEL\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove Jira/Markdown formatting\n",
    "    text = re.sub(r'\\{[^}]*\\}', '', text)  # Remove Jira macros\n",
    "    text = re.sub(r'\\[.*?\\|.*?\\]', '', text)  # Remove Jira links\n",
    "    text = re.sub(r'!.*!', '', text)  # Remove image references\n",
    "    \n",
    "    # Convert to plaintext\n",
    "    text = re.sub(r'[*_~^]+', '', text)  # Remove formatting chars\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split text into smaller chunks for embedding.\"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return [text] if text else []\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding vector for a text\"\"\"\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return None\n",
    "    \n",
    "    if model == MODEL_OPENAI:\n",
    "        response = openai.Embedding.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )[\"data\"][0][\"embedding\"]\n",
    "    else:\n",
    "        response = ollama.embed(\n",
    "            model='nomic-embed-text',\n",
    "            input=text\n",
    "        ).embeddings[0]\n",
    "\n",
    "    return response\n",
    "\n",
    "def process_jira_issues():\n",
    "    \"\"\"Process Jira issues, generate chunks and embeddings, and store them.\"\"\"\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all issues that don't have embeddings yet\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT i.issue_key, i.summary, i.description\n",
    "    FROM jira_issues i\n",
    "    LEFT JOIN embeddings e ON e.source_id = i.issue_key AND e.source_type = 'issue'\n",
    "    WHERE e.id IS NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    issues = cursor.fetchall()\n",
    "    \n",
    "    for issue_key, summary, description in issues:\n",
    "        # First, clean and prepare the text content\n",
    "        clean_description = clean_text(description)\n",
    "        \n",
    "        # Split the content into chunks\n",
    "        content_text = f\"Issue: {summary}\\n\\nDescription: {clean_description}\"\n",
    "        chunks = chunk_text(content_text)\n",
    "        \n",
    "        # Process each chunk and prepend the issue key to EACH chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Add issue key to each individual chunk\n",
    "            chunk_with_key = f\"Issue Key: {issue_key} | {chunk}\"\n",
    "            \n",
    "            # Generate embedding\n",
    "            embedding = generate_embedding(chunk_with_key)\n",
    "            \n",
    "            if embedding:\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO embeddings \n",
    "                    (source_type, source_id, content, embedding, chunk_number)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                    ON CONFLICT (source_type, source_id, chunk_number) \n",
    "                    DO UPDATE SET content = EXCLUDED.content, embedding = EXCLUDED.embedding\n",
    "                    \"\"\",\n",
    "                    ('issue', issue_key, chunk_with_key, embedding, i)\n",
    "                )\n",
    "        \n",
    "        conn.commit()\n",
    "    print(f\"Embedded {len(issues)} issues\")\n",
    "\n",
    "def process_jira_comments():\n",
    "    \"\"\"Process Jira comments, generate chunks and embeddings, and store them.\"\"\"\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all comments that don't have embeddings yet\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT c.id, c.issue_key, c.author, c.body\n",
    "    FROM jira_comments c\n",
    "    LEFT JOIN embeddings e ON e.source_id = CAST(c.id AS TEXT) AND e.source_type = 'comment'\n",
    "    WHERE e.id IS NULL AND LENGTH(c.body) > 50\n",
    "    \"\"\")\n",
    "    \n",
    "    comments = cursor.fetchall()\n",
    "    \n",
    "    for comment_id, issue_key, author, body in comments:\n",
    "        # Clean text\n",
    "        clean_body = clean_text(body)\n",
    "        \n",
    "        # Format comment content\n",
    "        content_text = f\"Comment by {author}:\\n{clean_body}\"\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = chunk_text(content_text)\n",
    "        \n",
    "        # Process each chunk and prepend the issue key to EACH chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Add issue key to each individual chunk\n",
    "            chunk_with_key = f\"Issue Key: {issue_key} | {chunk}\"\n",
    "            \n",
    "            # Generate embedding\n",
    "            embedding = generate_embedding(chunk_with_key)\n",
    "            \n",
    "            if embedding:\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO embeddings \n",
    "                    (source_type, source_id, content, embedding, chunk_number)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                    ON CONFLICT (source_type, source_id, chunk_number) \n",
    "                    DO UPDATE SET content = EXCLUDED.content, embedding = EXCLUDED.embedding\n",
    "                    \"\"\",\n",
    "                    ('comment', str(comment_id), chunk_with_key, embedding, i)\n",
    "                )\n",
    "        \n",
    "        conn.commit()\n",
    "    print(f\"Embedded {len(comments)} comments\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_jira_issues()\n",
    "#     process_jira_comments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 10 issues\n",
      "Embedded 10 comments\n"
     ]
    }
   ],
   "source": [
    "process_jira_issues()\n",
    "process_jira_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import openai\n",
    "import ollama\n",
    "from src.utils.constants import MODEL_OPENAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = os.getenv(\"MODEL\")\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    \"\"\"Generate embedding vector for a query.\"\"\"\n",
    "    if model == MODEL_OPENAI:\n",
    "        response = openai.Embedding.create(\n",
    "            input=query,\n",
    "            model=\"query-embedding-ada-002\"\n",
    "        )[\"data\"][0][\"embedding\"]\n",
    "    else:\n",
    "        response = ollama.embeddings(\n",
    "            model='nomic-embed-text',\n",
    "            prompt=query\n",
    "        ).embedding\n",
    "\n",
    "    return response\n",
    "\n",
    "def retrieve_relevant_context(query, limit=5):\n",
    "    \"\"\"Retrieve relevant context from the vector database based on query similarity.\"\"\"\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = generate_query_embedding(query)\n",
    "    \n",
    "    # Perform vector similarity search\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            e.source_type, \n",
    "            e.source_id, \n",
    "            e.content,\n",
    "            (e.embedding <=> %s) as distance\n",
    "        FROM embeddings e\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT %s\n",
    "        \"\"\",\n",
    "        (query_embedding, limit)\n",
    "    )\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    # todo: Better context specific to issue (current comments context is bs) + related to similar issue\n",
    "    # Format results as context\n",
    "    context = []\n",
    "    for source_type, source_id, content, distance in results:\n",
    "        # Get additional information based on source type\n",
    "        if source_type == 'issue':\n",
    "            issue_info = get_issue_info(source_id)\n",
    "            context_str = f\"JIRA ISSUE {source_id} - {issue_info['summary']} (Status: {issue_info['status']}):\\n{content}\"\n",
    "        elif source_type == 'comment':\n",
    "            comment_info = get_comment_info(source_id)\n",
    "            context_str = f\"COMMENT on {comment_info['issue_key']} by {comment_info['author']}:\\n{content}\"\n",
    "        else:\n",
    "            context_str = f\"{source_type.upper()} {source_id}:\\n{content}\"\n",
    "        \n",
    "        context.append(context_str)\n",
    "    \n",
    "    return \"\\n\\n\".join(context)\n",
    "\n",
    "def get_issue_info(issue_key):\n",
    "    \"\"\"Get basic information about a Jira issue.\"\"\"\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT summary, status\n",
    "        FROM jira_issues\n",
    "        WHERE issue_key = %s\n",
    "        \"\"\",\n",
    "        (issue_key,)\n",
    "    )\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if result:\n",
    "        return {\n",
    "            \"summary\": result[0],\n",
    "            \"status\": result[1]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"summary\": \"Unknown issue\",\n",
    "            \"status\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "def get_comment_info(comment_id):\n",
    "    \"\"\"Get basic information about a Jira comment.\"\"\"\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT issue_key, author\n",
    "        FROM jira_comments\n",
    "        WHERE id = %s\n",
    "        \"\"\",\n",
    "        (comment_id,)\n",
    "    )\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if result:\n",
    "        return {\n",
    "            \"issue_key\": result[0],\n",
    "            \"author\": result[1]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"issue_key\": \"Unknown\",\n",
    "            \"author\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "\n",
    "def generate_rag_response(query):\n",
    "    \"\"\"Generate a response using RAG methodology.\"\"\"\n",
    "    # Retrieve relevant context\n",
    "    context = retrieve_relevant_context(query)\n",
    "    \n",
    "    # todo: different system_prompt for user vs support\n",
    "    # Construct prompt with context\n",
    "    system_prompt = \"\"\"\n",
    "    You are a frontdesk assistant for Capital Area Food Bank (CAFB). Various partners of CAFB come to you with queries/complaints/questions.\n",
    "    You should help these partners by understanding their query and providing helpful answers. \n",
    "    These questions might be related to existing Jira tickets or might require creation of new Jira tickets\n",
    "    Use the provided context from the Jira database to answer the questions.\n",
    "    If the answer cannot be found in the context, say so clearly and suggest \n",
    "    how the user might refine their question.\n",
    "    \"\"\"\n",
    "    # todo: get actual jira issue key from user?\n",
    "    user_prompt = f\"\"\"\n",
    "    Context information from Jira:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    if model == MODEL_OPENAI:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=1000\n",
    "        ).choices[0].message.content\n",
    "    else:\n",
    "        response = 0\n",
    "    \n",
    "    return response\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import argparse\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='Test RAG retrieval system')\n",
    "#     parser.add_argument('--query', required=True, help='Query to test')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     print(f\"Query: {args.query}\")\n",
    "#     print(\"\\nGenerated Response:\")\n",
    "#     print(generate_rag_response(args.query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatusResponse(status='success')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.copy('gemma3', destination='user/localllm/gemma3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def chat_with_ollama():\n",
    "    # Initialize the conversation history\n",
    "    messages = []\n",
    "    \n",
    "    print(\"Starting chat with Gemma3 (type 'exit' to end the conversation)\")\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        \n",
    "        # Check if user wants to exit\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"\\nEnding conversation. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Add user message to history\n",
    "        messages.append({\n",
    "            'role': 'user',\n",
    "            'content': user_input\n",
    "        })\n",
    "        \n",
    "        # Get response from Ollama\n",
    "        print(\"\\nGemma3: \", end=\"\", flush=True)\n",
    "        \n",
    "        # Stream the response\n",
    "        stream = ollama.chat(\n",
    "            model='gemma3',\n",
    "            messages=messages,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        # Process and display the streamed response\n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk['message']['content']\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "        \n",
    "        # Add assistant's response to the conversation history\n",
    "        messages.append({\n",
    "            'role': 'assistant',\n",
    "            'content': full_response\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postgresql://sally:sallyspassword@dbserver.example:5555/userdata?connect_timeout=10&sslmode=require&target_session_attrs=primary\n",
    "    ^          ^         ^               ^           ^     ^          ^\n",
    "    |- schema  |         |- password     |- host     |     |          |- parameter list\n",
    "               |                                     |     |\n",
    "               |- username                           |     |- database\n",
    "                                                     |\n",
    "                                                     |- port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Config\n",
    "POSTGRES_HOST=localhost\n",
    "POSTGRES_PORT=5432\n",
    "POSTGRES_DB=jira_rag\n",
    "POSTGRES_USER=postgres\n",
    "POSTGRES_PASSWORD=your_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshavardhan-patil/Work/Projects/cafb-ai/.venv/lib/python3.11/site-packages/langchain_community/utilities/sql_database.py:134: SAWarning: Did not recognize type 'vector' of column 'embedding'\n",
      "  self._metadata.reflect(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3\",\n",
    ")\n",
    "\n",
    "db = SQLDatabase.from_uri(\"postgresql://postgres:your_password@localhost:5432/jira_rag\")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['02/25/2025 11:24;712020:b15fa988-bc60-49b4-aa6f-e30b7a3629c1;Good morning,\\n\\nThank you for contacting Customer Support. We have received your request, are looking into it, and will follow up with you shortly.\\n\\nCharity \\n02/25/2025 01:35;61536ea272f6970069fc1dbd;Good afternoon!\\n\\nThank you for submitting your inquiry. I have contacted the Maryland Regional Team and reviewed the documentation provided for the grant. Unfortunately, the grant can only be applied to orders with a creation date after February 10th, which is when the funds were uploaded to your accounts. Therefore, we cannot apply the grant to any orders created before February 10th, even if they were delivered after that date.\\n\\nIf you have any questions, please let me know.\\n\\nBest,  \\n\"D\"\\n02/25/2025 04:02;qm:65d20c5a-0c84-458e-8ce1-adb90d07460f:2913356c-1542-41f5-b091-dfbcb958adb0;Thanks for the clarification, \"D\".\\n\\nLinda\\n02/25/2025 04:03;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;Customer has replied after the ticket has been resolved.']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = connect_to_db()\n",
    "cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "SELECT body FROM jira_comments        \n",
    "WHERE issue_key = %s\n",
    "\"\"\",\n",
    "('CAFBSS-443',)\n",
    ")\n",
    "\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
